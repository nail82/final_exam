\documentclass{report}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage[dvips]{graphicx}
\usepackage{ifthen}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{upgreek}
\usepackage{listings}
\usepackage[left=0.75in, top=1in, right=0.75in, bottom=1in]{geometry}

\lstloadlanguages{Python}
\lstset{language=Python,
        frame=single,
       }
% Homework Specific Information
\newcommand{\hmwkTitle}{Final Exam}
\newcommand{\hmwkSubTitle}{}
\newcommand{\hmwkDueDate}{10 December 2012}
\newcommand{\hmwkClass}{CS 640}
\newcommand{\hmwkClassTime}{2:20}
\newcommand{\hmwkClassInstructor}{Dr Ranganath}
\newcommand{\hmwkAuthorName}{Ted Satcher}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{}
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\title{\vspace{2in}\textmd{\textbf{\hmwkClass:\ \hmwkTitle\ifthenelse{\equal{\hmwkSubTitle}{}}{}{\\\hmwkSubTitle}}}\\\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}\vspace{3in}}
\date{}
\author{\textbf{\hmwkAuthorName}}

\begin{document}
\maketitle

\begin{abstract}
  This report describes my work on the CS640 Final Exam, Fall Semster
  2012.
\end{abstract}


\section*{Problem 1 Batch Perceptron}
The batch perceptron algorithm is a trainable classifier used to
calculate a decision boundary between sample patterns of linearly
separable categories.  The problem consists of two different tasks
of finding decision boundaries.

My preparation for solving the batch perceptron began with
transcribing the three category sample vectors from the book into two
comma delimited text files.  One file consists of the sample vectors
for $\omega_1$ and $\omega_2$, and the other contains the samples for
$\omega_2$ and $\omega_3$.

The code for my solution to Problem 1 consists of two files.  The
executable portion is in problem1.py and the batch perceptron
algorithm and supporting functions are in batch\_perceptron.py.

\subsection*{Part a}
Separating the $\omega_1$ and $\omega_2$ categories requried 24
iterations.  A plot of the categories and calculated decision boundary
is in Figure~\ref{fig:prob1a-batch}.

\begin{figure}
\centering
  \includegraphics[scale=0.5]{images/prob1a.eps}
  \label{fig:prob1a-batch}
\end{figure}

\subsection*{Part b}
Categories $\omega_2$ and $\omega_3$ separated in 17 iterations of the
batch perceptron algorithm.  The plot of those categories and
associated decision boundary is in Figure~\ref{fig:prob1b-batch}.
\begin{figure}
\centering
  \includegraphics[scale=0.5]{images/prob1b.eps}
  \label{fig:prob1b-batch}
\end{figure}

\subsection*{Part c}
% Need to sort out why.  Run a couple of experiments based on
% cluster separation, min distance, and maybe monkey with
% the initial weight vector to see if that changes things.

\section*{Problem 2 Even Parity}
This problem requried training a back propagation neural
network to classify four digit binary numbers into even and odd
parity categories.

% Experiment with differing numbers of hidden units to see
% how/if they converge.  Think of a way to plot the decision
% boundary.

\section*{Problem 3 Bayes Classifier}
% Write out the estimated mean vectors and cov matrices
% The straight distros seem to plot correctly, but the diff
%   of the discrim functions doesn't look right.  Not sure
%   what gives with that.  Might be a math error in discrim
%   func calculation.

\section*{Problem 4 Parzen Window}
The Parzen Window algorithm is used to estimate a probability density
function from a collection of sample vectors. The problem statement
asked us to estimate the density function that a set of observations
were draw from using the Parzen Window approach.  Increasing numbers
of samples were used to illustrate how the estimate of the
density function improved.

% Although I can conceive of algorithms to calculate and plot the
% results of the Parzen Window approach, I felt it important to
% structure my code in such a way as to calculate and return an
% estimated probability distribution function.  Python is suited for
% this type of programming as it is well provisioned with functional
% programming facilities.

Using the numpy statistical sampling function, I generated 10000
observations from a normal distribution with parameters $N(3,4)$.  A
histogram of these data is in Figure~\ref{fig:p4-hist} which illustrates
the underlying distribution.

The executable code for this problem is in problem4.py and the
supporting functions are found in parzen\_window.py.

\begin{figure}
\centering
  \includegraphics[scale=0.5]{images/p4-hist.eps}
  \label{fig:p4-hist}
\end{figure}

\subsection*{Part a}
The instructions for part a required the use of the zero-one window
function in Equation 9, page 164 of the textbook.  Figure
~\ref{fig:zero-one-parzen} overplots the three estimates of the
density function using 100, 1000 and 10000 samples from the
distribution.  The estimation with a small number of samples
and a non-continuous window function produces a poor estimation.
This can clearly be seen in Figure~\ref{fig:zero-one-parzen}.

\begin{figure}
\centering
  \includegraphics[scale=0.5]{images/zero-one.eps}
  \label{fig:zero-one-parzen}
\end{figure}

\subsection*{Part b}
The density function was again estimated in this section using varying
numbers of samples.  The Gaussian window function was utilized in this
section producing estimates with more smooth characteristics.  My
results are illustrated in Figure~\ref{fig:gauss-parzen}.
\begin{figure}
\centering
  \includegraphics[scale=0.5]{images/gauss.eps}
  \label{fig:gauss-parzen}
\end{figure}

\section*{Problem 5 K-means}
K-means is a clustering algorithm.

My implementation is in two files, problem5.py is the executable and
kmeans.py is the algorithm functions.

Built a simple set of clustered patterns.  Shuffle the indexes of the
cluster patterns to assign the initial centers.  Noticed a tendency
that one cluster center would collect no patterns.  My first fix was
to pick a pattern, but then the cluster center would get stuck on that point.

\end{document}